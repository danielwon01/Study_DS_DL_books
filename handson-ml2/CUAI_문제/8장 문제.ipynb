{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c126e66",
   "metadata": {},
   "source": [
    "##  차원의 저주란 무엇인가요?\n",
    "\n",
    "차원의 저주란 차원이 증가하면서 학습데이터 수가 차원 수보다 적어져서 성능이 저하되는 현상을 일컫는다. 차원이 증가할수록 변수가 증가하고, 개별 차원 내에서 학습할 데이터 수가 적어진다. \n",
    "\n",
    "이때 주의할 점은 변수가 증가한다고 반드시 차원의 저주가 발생하는 것은 아니다. 관측치보다 변수 수가 많아지는 경우에 차원의 저주 문제가 발생하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb2f0",
   "metadata": {},
   "source": [
    "## 고차원 데이터셋이 위험한 이유는 무엇인가요?\n",
    "\n",
    "고차원은 훨씬 더 많은 공간을 가지고 있기 때문에 직관적이진 않습니다. \n",
    "\n",
    "이로 인해 고차원의 데이터셋은 보통 데이터들끼리 멀리 떨어져 가능성이 높다는 걸 유추해볼 수 있습니다. 이러한 경우 예측을 위한 훨씬 더 많은 외삽(extrapolation : 관찰이 어려운 데이터에 대해 추측하는 것)을 요구하기 때문에 불안정해집니다. 우리는 보통 이것을 고차원일수록 Overfitting 위험이 크다고 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a55930",
   "metadata": {},
   "source": [
    "## 외삽(extrapolation)이란 무엇인가요?\n",
    "\n",
    "외삽법이란 이전의 경험에 비추어, 보다 과학적인 맥락에서는 이전의 실험으로부터 얻은 데이터들에 비추어, 아직 경험/실험하지 못한 경우를 예측해보는 기법이다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dee10b",
   "metadata": {},
   "source": [
    "## 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것은 왜 차원의 저주를 해결하는 방법이 될 수 없나요?\n",
    "\n",
    "차원의 저주를 해결하기 위한 이론적인 해결법은 고차원에서도 데이터 끼리의 거리가 가까울 수 있도록 즉 밀도가 높아질 때까지 dataset의 크기를 키우는 것입니다. 그러나 일정한 밀도에 도달하기까지 필요한 데이터 수는 차원 수가 커짐에 따라 기하급수적으로 늘어나기 때문에 현실적으로 어렵습니다.\n",
    "\n",
    "예를 들어 크기가 1인 2차원 평면에 0.1 거리 이내에 훈련 샘플을 모두 놓으려면 최소한 10 x 10 개의 샘플이 필요하다. 이를 100개의 차원으로 확장하면 10^100개의 훈련 샘플이 필요하다. 그렇기 때문에 현실적으로는 PCA와 같은 차원축소 기법을 이용해서 차원의 저주 문제를 해결해야한다. \n",
    "\n",
    "특성 수를 크게 줄여 차원축소를 하면 훈련 속도가 빨라지고 시각화 하기에도 좋다. 다만 일부 정보가 유실되기 때문에 성능도 조금 나빠질 수 있다. 따라서 무작정 차원 축소를 하기 보다는 원본 데이터로 훈련해서 시간이 오래 걸리는지 확인하고 차원축소 필요 여부를 결정할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb89f8",
   "metadata": {},
   "source": [
    "## 투영이란 무엇이고 문제점은 무엇인가요?\n",
    "\n",
    "투영이란 사영이라고도 하며, 평면(또는 공간) 위에 있는 도형의 각 꼭짓점을 평면(또는 공간) 위에 수직으로 내리고 각각 내린 점과 원래 점을 이어 직선을 긋는 것을 말한다. 즉, 형을 평면(또는 공간)에 옮기는 것이다. 쉽게 이해하려면 어떤 도형 위에 수직으로 빛이 들어와 어떤 평면이나 공간에 생기는 그림자를 생각하면 된다.\n",
    "\n",
    "대부분의 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않다. 결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 부분공간,subspace에 놓여 있다. 모든 훈련 샘플을 부분 공간에 수직으로 투영하면 3D고차원 데이터를 2D로 만들어 축소하여 사용할 수 있다. 하지만 투영이 언제나 최선의 방법은 아니다. 부분 공간이 뒤틀리거나 휘어 있는 경우 투영을 이용하면 층이 서로 뭉개지게 될 수 있기 때문이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ba457",
   "metadata": {},
   "source": [
    "## 매니폴드와 매니폴드 학습이란 무엇인가요?\n",
    "\n",
    "일반적으로 d차원 매니폴드는 국부적으로 d차원 표평면으로 보일 수 있는 n차원 공간의 일부이다.\n",
    "\n",
    "매니폴드 학습 이란 고차원데이터가 있을 때 고차원 데이터를 데이터 공간에 뿌리면 샘플들을 잘 아우르는 subpsace가 있을 것이라 가정에서 학습을 진행하는 방법. \n",
    "\n",
    "\n",
    "* 매니폴드 는 고차원의 데이터를 저차원으로 옮길 때 데이터를 잘 설명하는 집합의 모형\n",
    "* 매니폴드 학습이란 비선형 차원 축소에 관한 접근\n",
    "* 높은 차원에서 낮은 차원으로 변환하는 것을 임베딩이라 하며 그것에 대한 학습 과정을 매니폴드 학습이라 함\n",
    "* 고차원 공간 중에 존재하는 실질적으로 보다 저차원으로 표시 가능한 도형\n",
    "\n",
    "> 특징 \n",
    "매니폴드는 고차원 데이터를 잘 표현하고 이는 데이터의 중요한 특징을 발견하는 것  \n",
    "고차원 데이터의 매니폴드 좌표들을 조정해보면 manifold의 변화에 따라 학습 데이터도 유의미하게 조금씩 변하는 것을 확인가능  \n",
    "데이터(샘플)을 잘 아우르는 매니폴드 를 찾게 되면 feature를 잘 찾았기 때문에 매니폴드의 좌표를 조금씩 변경해가면서 데이터를 유의미하게 조금씩 변화시킬 수 있다. 데이터의 dominant한 feature를 잘 찾았기 때문이다. \n",
    "역으로 매니폴드 를 잘 찾았다면 dominant feature가 유사한 sample들을 찾을 수 있다. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e458c",
   "metadata": {},
   "source": [
    "## PCA란 무엇이고 주성분이란 무엇인가요?\n",
    "\n",
    "주성분 분석(Principal Component Analysis, PCA)은 가장 널리 사용되는 차원 축소 기법 중 하나로, 원 데이터의 분포를 최대한 보존하면서 고차원 공간의 데이터들을 저차원 공간으로 변환한다.\n",
    "\n",
    "PCA는 훈련 세트에서 분산이 최대인 축을 찾는다. 고차원 데이터셋이라면 PCA는 이전의 두축에 직교하는 축을 찾으며 데이터셋에 있는 차원의 수만 큼 n번째 축을 찾는다. i번째 축을 이 데이터의 i번째 주성분이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebedeba",
   "metadata": {},
   "source": [
    "## PCA는 어떤 방식을 통해 주성분을 찾고 d차원으로 투영할 수 있나요?\n",
    "\n",
    "PCA는 훈련 세트에서 분산이 최대인 축을 찾는다. 고차원 데이터셋이라면 PCA는 이전의 두축에 직교하는 축을 찾으며 데이터셋에 있는 차원의 수만 큼 n번째 축을 찾는다. i번째 축을 이 데이터의 i번째 주성분이라고 부른다.\n",
    "각 주성분을 위해 PCA는 주성분 방향을 가리키고 원점에 중앙이 맞춰진 단위 벡터를 찾는다. 하나의 축에 단위 벡터가 반대 방향으로 두개 있으므로 PCA가 반환하는 단위 벡터의 방향을 일정하지 않다. 훈련세트를 조금 섞은 다음 다시 PCA를 적용하면 새로운 PC중 일부가 원래 PC와 반대 방향일 수 있다. 그러나 일반적으로 같은 축에 놓여 있을 것이다. 어떤 경우에는 한쌍의 PC가 회전하거나 서로 바뀔 수 있지만 보통은 같은 평면을 구성한다.\n",
    "\n",
    "주성분을 모두 추출해냈다면 처음 d개의 주성분으로 정의한 추평면에 투영하여 데이터셋의 차원을 d차원을 축소시킬 수 있다. 이 초평면은 분산을 가능한 최대로 보존하는 투영임을 보장한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9821e16",
   "metadata": {},
   "source": [
    "## 커널 PCA란 무엇이고 어떤 방식을 통해 좋은 커널과 하이퍼파라미터를 선택하나요?\n",
    "\n",
    "커널 PCA란 SVM에서 사용되는 샘플을 매우 높은 고차원 공간으로 암묵적으로 매핑하여 비선형 분류와 회귀를 가능하게 하는 수학적 기법인 커널 트릭을 PCA에 적용해 차원 축소를 위한 복잡한 비선형 투형을 실행한 방법이다.  이 기법은 투영된 후에 샘플의 군집을 유지하거나 꼬인 매니폴드에 가까운 데이터셋을 펼칠 때도 유용하다 .\n",
    "\n",
    "KPAC는 비지도 학습이기 때문에 좋은 커널과 하이퍼파라미터를 선택하기 위한 명확한 성능 측정 기준이 없다. 하지만 차원 축소는 종종 지도학습의 전처리 단계로 활용되므로 그리드 탐색을 사용하여 주어진 문제에서 성능이 좋은 커널과 하이퍼파라미터를 선택할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e66f3",
   "metadata": {},
   "source": [
    "## 데이터셋에 적용한 차원 축소 알고리즘의 성능은 어떻게 평가할 수 있을까요?\n",
    "\n",
    "측정하는 한 가지 방법은 역변환을 수행해서 재구성 오차를 측정하는 것이다.  하지만 모든 차원 축소 알고리즘이 역변환을 제공하지는 않습니다. 만약 차원 축소를 다른 단계로 사용한다면 두 번째 알고리즘의 성능을 측정해볼 수 있습니다. 즉, 차원 축소가 너무 많은 정보를 잃지 않았다면 원본 데이터셋을 사용했을 때와 비슷한 성능이 나와야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
