{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613f326c",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신(Support Vector Machine, SVM)은 무엇인가요?\n",
    "\n",
    "서포트 벡터 머신(SVM)은 결정 경계(Decision Boundary), 즉 분류를 위한 기준 선을 정의하는 모델이다. 그래서 분류되지 않은 새로운 점이 나타나면 경계의 어느 쪽에 속하는지 확인해서 분류 과제를 수행할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea2067",
   "metadata": {},
   "source": [
    "### 초평면이란?\n",
    "초평면은 평면을 나누는 경계입니다. 데이터 포인트를 2 개의 개별 클래스로 분류하는 결정 경계입니다. SVM은 데이터를 다차원으로 분류하는 데 사용되기 때문에 초평면은 입력이 2 개인 경우 직선이 될 수 있고 입력이 2 개 이상이면 2D 평면이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae76e17",
   "metadata": {},
   "source": [
    "### 서포트 벡터 란?\n",
    "서포트 벡터는 초평면을 최적화하는 데 도움이되는 데이터 포인트입니다. 이러한 벡터는 초평면에 가장 가깝고 분류하기가 가장 어렵습니다. 결정 초평면의 위치는 지원 벡터에 따라 다릅니다. 이러한지지 벡터를 제거하면 초평면의 위치도 변경됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca6f9b",
   "metadata": {},
   "source": [
    "### 하드 마진 분류(Hard Margin Classification)의 문제점은 무엇인가요?\n",
    "\n",
    "하드마진분류란 모든 샘플이 결정경계 바깥쪽에 올바르게 분류되어 있는 경우  \n",
    "하드마진분류의 문제점은 데이터가 선형적으로 구분될 수 있어야 제대로 작동하며, 이상치에 민감하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a6d3c",
   "metadata": {},
   "source": [
    "### 사이킷런의 SVM 모델을 만들 당시 하이퍼파라미터 중 C가 높고 낮음에 따라 어떤 결과가 도출되나요?\n",
    "\n",
    "SVM에서 중요한 하이퍼파라미터는 Gamma와 C가 있는데 Gamma는 가까이 있는 데이터에 얼마나 더 가중치를 줄것인가를 판단하는 것이고 C는 제약조건의 강도를 조절하는 것이다.  \n",
    "\n",
    "C값이 낮을 수록 강한 제약이 설정된다. 모델이 과적합된 경우 일반화 성능을 높이기 위해 사용된다. C 값을 줄이면 결정경계 폭이 넓어지지만 마진 오류도 줄어듭니다.\n",
    "\n",
    "C값이 높을 수록 약한 제약이 설정된다. 모델이 과소적합된 경우 학습 성능을 높이기 위해 사용된다. C 값을 늘이면 마진 오류가 줄어들지만 결정경계 폭이 줄어들게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c7573",
   "metadata": {},
   "source": [
    "#### svm 하이퍼파라미터\n",
    "* C\t오류를 얼마나 허용할 것인지 (규제항) 클수록 하드마진, 작을수록 소프트마진에 가까움\n",
    "* kernel(가우시안 커널)\t'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'\n",
    "* degree\t\t다항식 커널의 차수 결정\n",
    "* gamma\t결정경계를 얼마나 유연하게 그릴지 결정 클수록 오버피팅 발생 가능성 높아짐\n",
    "* coef0\t다항식 커널에 있는 상수항 r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361582da",
   "metadata": {},
   "source": [
    "### 비선형 특성을 다루는 방법에는 무엇이 있나요?\n",
    "\n",
    "다항특성과 같은 특성을 더 추가하는 방법이 있다. 선형적으로 구분되는 데이터셋이 만들어질 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279346e",
   "metadata": {},
   "source": [
    "### 가우시안 RBF 커널의 하이퍼파라미터 gamma는 무엇인가요?\n",
    "\n",
    "가우시안 RBF커널의 gamma는 결정경계를 얼마나 유연하게 그릴지 결정하는 것으로, gamma를 증가시키면 각 샘플의 영향 범위가 작아지고 결정경계가 조금 더 불규칙해지고 각 샘플을 따라 구불 구불하게 휘어진다. gamma를 줄이면 샘플이 넓은 범위에 걸쳐 영향을 주므로 결정경계가 더 부드러워진다. 모델이 과대적합일 경우엔 gamma를 감소시켜야 하고 과소적합일 경우 gamma값을 증가시켜야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcbebc",
   "metadata": {},
   "source": [
    "### Margin이란 \n",
    "마진이란 ‘결정 경계로부터 등간격으로 확장시켰을 때 가장 가까이 만나는 (양쪽 클래스의) 객체와의 거리’입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3c147",
   "metadata": {},
   "source": [
    "### 마진 오류(하드 마진)를 만들지 않으려면 어떻게 해야하나요?\n",
    "\n",
    "SVM의 목적은 마진을 크게 하는 W와 b의 값을 찾는 것이라고 했습니다. 결정 함수의 기울기는 가중치 벡터 ||w||의 노름과 같습니다. 가중치 w가 작을수록 마진은 커집니다. 음성 샘플일때 t(i)=-1로,양성 샘플 일 때 t(i)=1로 정의하면 제약 조건을 t(i)(wT+X(i)+b) >= 1로 표현할 수 있다.즉, minimize1/2wT·w가 되고 , t(i)(wT+X(i)+b) >= 1의 제약식을 가집니다. 단, 이것은 마진 오류를 하나도 만들지 않는 하드 마진에 속합니다. 그렇다면 오류를 약간은 범하되 마진을 늘리는 소프트 마진의 경우 목적 함수는 각 샘플에 대해 슬랙 변수를 도입해야 합니다. 즉,  minimize1/2wT·w + C∑ζ(i)가 되고  t(i)(wT+X(i)+b) >= 1-ζ(i)와 ζ(i)>=0의 제약을 가집니다. \n",
    "\n",
    "**마진오류를 하나도 만들지 않으려면, 결정함수가 모든 양성 훈련 샘플에서는 1보다 커야하고 음성훈련 샘플에서는 -1보다 작아야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de833c",
   "metadata": {},
   "source": [
    "### 선형 SVM 분류기 모델에서 결과값과 예측된 클래스 사이에는 어떤 관계가 있나요?\n",
    "\n",
    "선형 svm 분류기 모델은 단순히 결정함수를 계산해서 새로운 샘플 x의 클래스를 예측한다. 결과값이 0보다 크면 예측된 클래스 \n",
    "은 양성클래스(1)가 된다.그렇지 않으면 음성 클래스(0)가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04746f6",
   "metadata": {},
   "source": [
    "### 머신러닝에서 커널은 무엇인가요?\n",
    "\n",
    "SVC는 커널트릭 알고리즘을 구현한 libsvm라이브러리를 기반으로 한다. 훈련의 시간복잡도는 O(m 2 x n) 과 O(m 3 x n)사이 이다. 훈련샘플수가 커지면 엄청나게 느려진다는 것을 의미, 복잡하지만 작거나 중간규모의 훈련세트에 알고리즘이 맞는다. 하지만 특성의 개수에는 특히 회소특성(각 특성에 0이 아닌 특성이 몇개 없는 경우)인 경우에는 잘 확장된다. 이 경우 알고리즘의 성능이 샘플이 가진 0이 아닌 특성의 평균수에 거의 비례\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
