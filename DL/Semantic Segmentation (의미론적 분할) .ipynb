{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0e005d",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*cJ3oLJ2s8W_sCPmTuMgIlw.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "* Classification (분류): 인풋에 대해서 하나의 레이블을 예측하는 작업.   \n",
    "  AlexNet, ResNet, Xception 등의 모델   \n",
    "  \n",
    "  \n",
    "* Localization/Detection (발견): 물체의 레이블을 예측하면서 그 물체가 어디에 있는지 정보를 제공. 위치 정보의 형태는 주로 Bounding Box를 많이 사용합니다.\n",
    "\n",
    "    YOLO, R-CNN 등의 모델  \n",
    "    \n",
    "* object detection \n",
    "    일반적으로 Classification과 Localization을 동시에 수행합니다.\n",
    "  입력으로 주어진 이미지 안의 객체 위치(Localization)와 해당 객체의 종류(Classification)를 출력하는 Task입니다.\n",
    "------\n",
    "\n",
    "\n",
    "    \n",
    "* Segmentation (분할): 모든 픽셀의 레이블을 예측, Segmentation은 픽셀을 대상으로 한 Classification 문제로 접근할 수 있습니다.\n",
    "입력으로 주어진 이미지 내에서 각 픽셀이 어떤 클래스에 속하는지 분류합니다.   \n",
    "\n",
    "각 픽셀의 분류된 클래스는 모델이 생성한 결과물인 예측 마스크 (mask)에 픽셀 단위로 기록됩니다. 만일 특정 픽셀이 어떤 클래스에도 해당하지 않는 경우, Background 클래스로 규정해 0을 표기하는 방식을 사용합니다.    \n",
    "\n",
    "분할의 기본 단위를 무엇으로 설정하느냐에 따라, Segmentation의 세부 문제를 나눌 수 있습니다.  \n",
    "\n",
    "Instance Segmentation\n",
    "분할의 기본 단위를 사물로 설정한 분할 문제입니다.\n",
    "만일 두 개 이상의 사물이 동일한 클래스에 속하더라도, 서로 다른 사물에 해당하면 이들은 서로 다른 예측 마스크값을 가집니다.  \n",
    "\n",
    "Semantic Segmentation \n",
    "분할의 기본 단위를 클래스로 설정한 분할 문제입니다.\n",
    "만일 두 개 이상의 사물이 동일한 클래스에 해당하면 이들은 서로 같은 예측 마스크값을 가집니다.\n",
    "* FCN, SegNet, DeepLab 등의 모델\n",
    "    \n",
    "이미지 내에 객체 존재 여부를 예측하는 문제(이미지 분류; Image Classification)에 비해서 객체 경계 정보를 보존해야하고 전체적인 이미지의 문맥을 파악해야 하는 등 조금 더 높은 수준의 이미지 이해를 요구한다는 점에서 까다로운 문제에 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99fba5",
   "metadata": {},
   "source": [
    "<img src=\"https://images.velog.io/images/cosmicdev/post/80eb0f27-8690-44f7-9eaf-0083c0c5caa0/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-10-25%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%203.02.45.png\n",
    "\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Semantic Image Segmentation의 목적은 사진에 있는 모든 픽셀을 해당하는 (미리 지정된 개수의) class로 분류하는 것이다. 이미지에 있는 모든 픽셀에 대한 예측을 하는 것이기 때문에 desce prediction이라고도 불린다.    \n",
    "\n",
    "주의할 점은 Semantic Image Segmentation은 같은 class의 instance를 구별하지 않는다는 것이다. class에 속하는 object이 있을 때 따로 분류하지 않고 그 픽셀 자체가 어떤 class에 속하는지에만 관심이 있다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8e314",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*p-F6uIUf6jmcM8VcVQarGQ.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "인풋으로 RGB이미지 (H X W X 3) 또는 흑백이미지 (H X W X 1) , 아웃풋으로는 각 픽셀별 어느 class에 속하는 나타내는 레이블을 나타낸 Segmentation Map. \n",
    "\n",
    "-----\n",
    "\n",
    "<img src=\" https://miro.medium.com/max/1400/1*lZEdphf72ZMwOhsDq-eEfg.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "One-Hot-encoding으로 각 class에 대해 출력채널을 만들어서 segmentation map을 만든다. class의 개수 만큼 만들어진 채널을 argmax를 통해서 위에 이미지 처럼 하나의 출력물을 도출한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf0177",
   "metadata": {},
   "source": [
    "#### Semantic Segmentation 방법들 \n",
    "\n",
    "AlexNet, VGG 등 분류에 자주 쓰이는 깊은 신경망들은 Semantic Segmentation 을 하는데 적합하지 않다. 일단 이런 모델은 parameter 의 개수와 차원을 줄이는 layer 를 가지고 있어서 자세한 위치정보를 잃게 되기 때문이다. 또한 보통 마지막에 쓰이는 Fully Connected Layer에 의해서 위치에 대한 정보를 잃게 된다.\n",
    "\n",
    "만약 공간/위치에 대한 정보를 잃지 않기 위해서 Pooling 과 Fully Connected Layer 를 없애고 stride 가 1이고 Padding 도 일정한 Convolution 을 진행할 수도 있을 것이다. 인풋의 차원은 보존하겠지만, parameter 의 개수가 많아져서 메모리 문제나 계산하는데 비용이 너무 많이 들어서 현실적으로는 불가능 할 것이다. \n",
    "\n",
    "이 문제의 중간점을 찾기 위해 보통 Semantic Segmentation 모델들은 보통 Downsampling 과 Upsampling의 형태를 가지고 있다. \n",
    "\n",
    "\n",
    "<img src=\" https://miro.medium.com/max/3020/1*3xgDGoeO36zjycZFHFgvCA.png\" width=\"600\" height=\"300\"/>  \n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "* Downsampling - 주 목적은 차원을 줄여서 적은 메모리로 깊은 Convolution 을 할 수 있게 하는 것이다. 보통 stride 를 2 이상으로 하는 Convolution 을 사용하거나, pooling을 사용한다. 이 과정을 진행하면 어쩔 수 없이 feature 의 정보를 잃게된다. \n",
    "\n",
    "\n",
    "* 마지막에 Fully-Connected Layer를 넣지 않고, Fully Connected Network 를 주로 사용한다. FCN 모델에서 위와같은 방법을 제시한 후 이후에 나온 대부분의 모델들에서 사용하는 방법이다. \n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "* Upsampling - Downsampling 을 통해서 받은 결과의 차원을 늘려서 인풋과 같은 차원으로 만들어 주는 과정이다. 주로 Strided Transpose Convolution 을 사용한다.\n",
    "\n",
    "\n",
    "* 논문들에서 Downsampling 하는 부분을 인코더, Upsampling 하는 과정을 디코더이라고 부른다.인코더를 통해서 입력받은 이미지의 정보를 압축된 벡터에 표현하고, 디코더를 통해서 원하는 결과물의 크기로 만들어낸다.\n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fci9MKy%2FbtreKJqrX8W%2FzHwYK0RFtyGk1xKOB2HRkK%2Fimg.png\n",
    "\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "* Nearest Neighbor Unpooling (위의 예시에너는 2X2 stride) : 해당 receptive field 의 모든 영역에 값을 복사하여 upsampling 하는 방법\n",
    "\n",
    "* Brd of Nails : receptive field 마다 특정 한 위치에만 값을 복사하고 나머지는 0 으로 채워넣어 upsampling 하는 방법\n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpjGvY%2FbtreJh9uWFD%2FIURKuNtSle5K9nk8GRr89k%2Fimg.png\n",
    "\" width=\"400\" height=\"300\"/>  \n",
    "\n",
    "* Max Unpooling\n",
    "   네트워크가 대칭적인 것을 이용한 방법으로 pooling과 연관을 짓는 것,downsampling 시 max pooling에 사용했던 요소들을 기억, upsampling 시 같은 자리에 값을 넣는 것이 아니라 maxpooling에 선택된 위치에 맞게 넣어주고 나머지는 0을 넣어준다.  \n",
    "   \n",
    "   \n",
    "  \n",
    "* 위 방법이 효과적인 이유\n",
    "   모든 픽셀의 카테고리를 잘 결정하는 것이 중요하고 결과에서 객체들 간의 디테일한 경계가 명확할수록 좋다. max pooling과 같은 방법을 시행 시에 특정 맵의 비균질성이 발생한다.,max pooling후 특정 맵이 어느 receptive field에서 왔는지 알게 되면 더 공간적인 정보를 잘 기억할 수 있게 된다.\n",
    "\n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcI734p%2FbtreKmoK23g%2FMKtFmlBxKu9BaThDEKxm80%2Fimg.png\n",
    "\" width=\"400\" height=\"300\"/>  \n",
    "\n",
    "* transpose convolution\n",
    "  입력이 2X2이고 출력이 4X4이다. 연산과정은 내적을 구하는 것이 아니라 입력값(가중치) X 필터의 값을 구하여 출력값에 넣어주는 것이다. stride가 2일 때 이므로 픽셀을 두 칸 이동하므로 이를 위의 슬라이드에서 빨간색 필터로부터 파란색 필터로 옮긴 것과 같다. 이때 필터부분이 receptive field인데 이 과정에서 두 부분이 겹치게 되는데 이는 간단하게 더해준다. 이 과정을 반복하여 upsampling 과정을 수행하는 것이다.     \n",
    "  \n",
    "  input의 빨간색 원소를 3x3 kernel에 곱해서 output의 대응하는 자리에 집어넣는다. 같은 방법으로 input의 파란색 원소를 3x3 kernel에 곱해서 output의 대응하는 위치에 집어넣는다. 이 때 output에 겹치는 구간(빗금 표시)이 발생하는데, 겹치는 부분의 값은 모두 더해준다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0d162",
   "metadata": {},
   "source": [
    "#### Fully Convolutional Network for Semantic Segmentation FCN \n",
    "\n",
    "\n",
    "\n",
    "FCN은 이미지 분류에서 우수한 성능을 보인 CNN 기반 모델(AlexNet, VGG16, GoogLeNet)을\n",
    "Semantic Segmentation Task를 수행할 수 있도록 변형시킨 모델,FCN 네트워크는 이미지 분류 문제를 먼저 트레이닝 시킨 후 모델을 튜닝해 학습하는 Transfer Learning으로 구현  ,이미지 영역을 나누고 독립적으로 classification 하는 방법이 아닌 전체 이미지를 통째로 모델에 넣어 학습하는 방법을 사용 \n",
    "\n",
    "\n",
    "전체 이미지가 input 으로 들어갔을때, 모든 픽셀에 대해 classification, 즉 class score 를 출력하려면 ouput 의 크기와 input 이미지의 크기가 동일해야한다. 이를 위해 FCN 은 fc-layer 가 없고 convolution layer 로 구성되어있다.\n",
    "\n",
    "전체 이미지가 네트워크의 input 으로 들어가 convolution layer, pooling layer 를 통해 downsampling 되며 작은 size를 가진 feature map 을 만들어내고, 특정 과정을 통해 작은 size의 feature map 을 upsampling 하여 최종적으로 input 이미지와 동일한 크기의 output 을 생성한다.\n",
    "이때 ouput 은 이미지의 각 픽셀별 class score 가 될 것이다.\n",
    "\n",
    "---------\n",
    "\n",
    "\n",
    "FCN은 기존 이미지 분류에서 쓰인 네트워크를 트레인된 상태에서(Pretrain)\n",
    "Feature Extraction 레이어는 그대로 재활용하여 Feature를 추출하고\n",
    "FC 레이어를 버리고 1X1 Conv 그리고 Up-sampling(Transpose Convolution)로 변경하여(Fine-Tuning)\n",
    "픽셀 클래스 분류와 입력이미지와 같은 사이즈 회복을 하도록 네트워크가 구성되었다. \n",
    "\n",
    "<img src=\" https://miro.medium.com/max/875/1*ddf0sJZDitiqVCWdjijzCA.png\" width=\"600\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "* 네트워크 구조 \n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdab59q%2Fbtq0tfkffFq%2Fk4GD8TKLusWjfVq36DMBn0%2Fimg.png\n",
    "\" width=\"600\" height=\"300\"/>  \n",
    "\n",
    "1. Convolution Layer를 통해 Feature 추출\n",
    "2. 1x1 Convolution Layer를 이용해 피처맵의 체널수를 데이터셋 객체의 개수와 동일하게 변경\n",
    "    (Class Presence Heat Map 추출)\n",
    "3. Up-sampling: 낮은 해상도의 Heat Map을 Upsampling(=Transposed Convolution) 한 뒤, 입력 이미지와 같은 크기의 Map 생성\n",
    "4. 최종 피처 맵과 라벨 피처맵의 차이를 이용하여 네트워크 학습\n",
    "\n",
    "\n",
    "\n",
    "----- \n",
    "* 문제점 존재 \n",
    "\n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbb7uvh%2Fbtq0zdkIsTF%2FQWkZ87uNnbUqjb505VldH0%2Fimg.png\" width=\"300\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "VGG16에서 입력 이미지의 크기가 224x224 인 경우 5개의 convolution block을 통과하면 Feature map의 크기는 7x7이 된다. 마찬가지로 FCN에서도 기존 입력 이미지 (크기 H x W)가 5개의 convolution block을 통과하면\n",
    "H/32 x W/32 크기의 Feature map을 얻게 된다.\n",
    "\n",
    "Feature map의 한 픽셀은 입력 이미지의 32 x 32 pixel를 대표하게 된다.\n",
    "이처럼 Feature map은 낮은 해상도를 가져 입력 이미지의 위치 정보를 '대략적으로만' 가지고 있다.\n",
    "\n",
    "문제는 3번 Up-sampling 과정에서 발생한다.\n",
    "입력 이미지 위치 정보를 '대략적으로' 가지고 있는 feature map을 Up-sampling하여 얻은 segmentation map은\n",
    "기존 입력 이미지와 비교했을 때 뭉뚱그려져 있고 디테일하지 못하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c07e6",
   "metadata": {},
   "source": [
    "#### Skip architecture\n",
    "FCN의 개발자들은 좀 더 디테일한 segmentation map을 얻기 위해, Skip architecture 라는 기법을 제안  \n",
    "\n",
    "앞에서 설명과 같이 최종 피처맵(Feature map)은 지역정보를 '대략적으로 유지'하고 있어 이미지가 뭉개지는 것을 보완한 방법이다. 피처 추출 단계의 피처맵도 업샘플링에 포함(Sum)하여 위치 정보 손실을 막자라는 것이 핵심 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d9b1c",
   "metadata": {},
   "source": [
    "#### FCN 확장 모델\n",
    "\n",
    "* FCN-32s  \n",
    "* 5번의 convolution block을 통과해 1/32만큼 줄어든 5번 Feature map(5번 Feature map 크기는 7x7)\n",
    "* 5번 Feature map이 convolution layer를 통과하여 같은 크기의 6번 Feature map을 얻음\n",
    "* 6번 Feature map을 한 번에 32배 upsampling (H/32 x W/32 크기를 H x W 크기로)\n",
    "\n",
    "*  FCN-16s\n",
    "* 4번의 convolution block을 통과해 1/16 만큼 줄어든 4번 Feature map, 4번 Feature map 크기는 14x14\n",
    "* 6번 Feature map을 2배 upsampling (H/32 x W/32 크기를 H/16 x W/16 크기로) 한 것과 4번 Feature map을 Sum 한다.(4-1번 Feature map)\n",
    "* 새롭게 얻은 4-1번 Feature map 을 한 번에 16배 upsampling (H/16 x W/16 크기를 H x W 크기로)\n",
    "\n",
    "\n",
    "<img src=\" https://wikidocs.net/images/page/143443/FCN-16S.png\" width=\"500\" height=\"300\"/>    \n",
    "\n",
    "\n",
    "* FCN-8s\n",
    "\n",
    "* FCN-16s과 유사한 Step이 추가됩니다.\n",
    "* 4-1번 Feature map을 2배 upsampling(H/16 x W/16 크기를 H/8 x W/8 크기로) 한 것과 3번 Feature map을 Sum한다.(3-1번 Feature map)\n",
    "* 3-1번 Feature map을 한 번에 8배 upsampling (H/8 x W/8 크기를 H x W 크기로)\n",
    "\n",
    "<img src=\" https://wikidocs.net/images/page/143443/FCN-8S.png\" width=\"500\" height=\"300\"/>    \n",
    "\n",
    "결과 이미지를 보면 위치 정보가 더 잘 전달되어 FCN-32s → FCN-16s → FCN-8s 순서로 더 정교해졌음을 알 수 있다. \n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FczpfKI%2Fbtq0vPkz77r%2F9vhp9EICvkrgtKTPrUtTM0%2Fimg.png\" width=\"300\" height=\"300\"/>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd1ef8",
   "metadata": {},
   "source": [
    "####  Dilated/Atrous Convolution 을 사용하는 방법\n",
    "\n",
    "Deeplab 이 제시하는 방법으로, 신호가 소멸되는것을 제어하고 다양한 크기의 특징을 익히는 방법을 제시  \n",
    "Atrous Convolution 은 dilation rate (확장 비율)이라는 새로운 변수를 사용한다. 이 비율은 커널에서 사용할 값들 사이에 얼마 만큼의 공간을 넣어줄 것인지에 대한 것이다. 커널 안에 공간이 없는 일반적인 convolution 은 확장비율은 1로 정의한다. 3 x 3 의 크기의 커널이 2의 확장비율을 갖는다면 실제로는 5 x 5 커널의 시야를 갖게 된다. 위와 같은 방법은 동일한 계산 비용으로 보다 넓은 시야를 갖을 수 있게 한다.  \n",
    "\n",
    "<img src=\" https://miro.medium.com/max/1400/1*bWchlYbT_81cQLzU8rOSSQ.png\" width=\"300\" height=\"300\"/>    \n",
    "\n",
    "\n",
    "----- \n",
    "\n",
    "Deeplab V3는 ImageNet에서 학습된 ResNet을 기본적인 특징 추출기로 사용한다. ResNet의 마지막 블럭에서는 여러가지 확장비율을 사용한 Atrous Convolution을 사용해서 다양한 크기의 특징들을 뽑아낼 수 있도록 한다.  \n",
    "  \n",
    "또한 이전 Deeplab 버젼에서 소개되었던 Atrous Spatial Pyramid Pooling (ASPP)을 사용한다. 새로운 아이디어라기 보다는, 좋은 성능을 보였던 모델들의 특징들을 섞어놓은 모델이다. 다양한 확장비율을 가진 커널을 병렬적으로 사용한 convolution 이다.\n",
    "\n",
    "<img src=\" https://miro.medium.com/max/1400/1*p6wpU7klyK-vGqISNumtlg.png\" width=\"600\" height=\"300\"/>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f75e9a",
   "metadata": {},
   "source": [
    "### U-Net \n",
    "\n",
    "* U-Net이란  \n",
    "   U-Net은 Semantic Segmentation Task 수행에 널리 쓰이는 모델 중 하나입니다.  \n",
    "   네트워크 형태가 알파벳 U와 비슷하다고 하여 붙여진 이름으로, 의생명공학 이미지 Segmentation을 위해 개발된 모델입니다.\n",
    "   \n",
    "   \n",
    "   <img src=\" https://tech.socarcorp.kr/img/car-damage-segmentation-model/unet-architecture.png\" width=\"500\" height=\"300\"/>    \n",
    "   \n",
    "\n",
    "* U-Net의 장점\n",
    "  U-Net은 기존의 Segmentation 모델의 문제점을 해결할 수 있습니다  \n",
    "  \n",
    "  \n",
    "1) 빠른 속도\n",
    "   \n",
    " * 기존 Segmentation 모델의 단점이었던 느린 연산 속도를 개선했습니다.  속도 개선이 가능했던 이유는, 이미지를 인식하는 단위(Patch)에 대한 Overlap 비율이 적기 때문입니다.  \n",
    " \n",
    " * 기존의 모델에서 많이 사용되었던 Sliding Window 방식의 경우, 이전 Patch에서 검증이 끝난 부분을 다음 Patch에서 다시 검증하게 됩니다. 이는 일종의 연산 낭비라고 볼 수 있습니다.  \n",
    "    U-Net의 경우, 이전 Patch에서 검증이 끝난 부분을 다음 Patch에서 중복하여 검증하지 않기 때문에, 연산의 낭비가 없고 이로 인해 향상된 속도를 얻을 수 있습니다.\n",
    "   \n",
    "   \n",
    "2) Context와 Localization의 늪에서 탈출    \n",
    "\n",
    "* Segmentation Network는 클래스 분류를 위한 인접 문맥 파악(Context)과 객체의 위치 판단(Localization)을 동시에 수행해야 합니다. 각 성능은 Patch의 크기에 영향을 받는데, 이때 Trade-Off 관계를 가지게 됩니다.    \n",
    "\n",
    "\n",
    "* Patch의 크기가 커지면 더 넓은 범위의 이미지를 한 번에 인식할 수 있어 Context 파악에는 탁월한 효과를 보이지만, 많은 Max-Pooling을 거치며 Localization 성능에는 부정적인 영향을 미치게 됩니다.  \n",
    "\n",
    "\n",
    "* 반대로 Patch의 크기가 작아지면 Localization 성능은 좋아지나, 인식하는 범위가 지나치게 협소해져 Context 파악 성능에 좋지 않은 영향을 미칩니다.  \n",
    "  U-Net은 다층의 Layer의 Output을 동시에 검증해서 이러한 Trade-Off를 극복합니다.\n",
    "   \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526d721",
   "metadata": {},
   "source": [
    "#### U-Net with EfficientNet\n",
    "\n",
    "U-Net의 구조는 알파벳 U의 왼쪽 절반에 해당하는 Contracting Path와 오른쪽 절반에 해당하는 Expanding Path의 2가지 Path로 분리할 수 있습니다.\n",
    "\n",
    "\n",
    " <img src=\" https://tech.socarcorp.kr/img/car-damage-segmentation-model/unet-architecture_path.png\" width=\"500\" height=\"300\"/>    \n",
    " \n",
    "* Contracting Path  \n",
    "  Contracting Path는 Encoder의 역할을 수행하는 부분으로 전형적인 Convolution Network로 구성됩니다.\n",
    "   \n",
    "   Contracting Path는 입력을 Feature Map으로 변형해 이미지의 Context를 파악합니다.\n",
    "    이 경우에 Contracting Path의 앞단에 이미 잘 학습된 모델을 Backbone으로 사용해 학습 효율과 성능을 높일 수 있습니다. 주로 ResNet 등의 모델을 사용합니다. 해당 프로젝트에서는 ImageNet 데이터 세트로 학습된 EfficientNet을 Contracting Path의 Backbone으로 사용했습니다. \n",
    "    \n",
    "\n",
    "* Expanding Path \n",
    "\n",
    "    Expanding Path는 Decoder의 역할을 수행하는 부분으로, 전형적인 Upsampling + Convolution Network로 구성됩니다.\n",
    "    즉, Convolution 연산을 거치기 전 Contracting Path에서 줄어든 사이즈를 다시 복원하는(Upsampling) 형태입니다.\n",
    "\n",
    "    Expanding Path에서는 Contracting을 통해 얻은 Feature Map을 Upsampling하고, 각 Expanding 단계에 대응되는 Contracting 단계에서의 Feature Map과 결합해서(Skip-Connection Concatenate) 더 정확한 Localization을 수행합니다.\n",
    "    즉, Multi-Scale Object Segmentation을 위하여 Downsampling과 Upsampling을 순서대로 반복하는 구조입니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
