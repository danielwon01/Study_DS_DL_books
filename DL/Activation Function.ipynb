{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0018f423",
   "metadata": {},
   "source": [
    "## 활성화 함수 속성 \n",
    "\n",
    "#### non-linear \n",
    "* 활성화 함수가 비선형인 경우 2개의 layer를 가진 신경망은 universal functoin approximator가 된다.  \n",
    "* The Universal Approximation Theorem tells us that Neural Networks has a kind of universality i.e. no matter what f(x) is, there is a network that can approximately approach the result and do the job\n",
    "* identity function은 선형이기 때문에 이러한 속성을 만족하지 못함, 전체 network는 하나의 layer를 가진 모델과 동일하게됨   \n",
    "\n",
    "-----\n",
    "\n",
    "#### Range\n",
    "* 활성화 함수의 범위가 유한한 경우, 패턴 표현을 제한된 가중치들에만 큰 영향을 주기 때문에 gradient기반 훈련 기법들은 더 안정적인 경향이 있다.\n",
    "* 반대로 범위가 무한한 경우, 패턴 표현은 대부분의 가중치들에게 큰 영향을 주기 때문에 훈련이 일반적으로 보다 효율적이게 됨, 이 경우 lr이 작아야함 \n",
    "\n",
    "-----\n",
    "\n",
    "#### Smooth functions with a monotonic derivate\n",
    "* graient가 monotonic derivative인 smooth function일 때 경우에 따라 일반화가 더 잘된다고 알려짐 \n",
    "* monotonic derivative - 특정함수를 나타내는 그래프의 gradient가 계속 증가 하는 경우에만 monotonic derivative하다고 표현, 어떤 입력의 gradient가 증가하는 구간이 있기 때문에 경향을 알 수 없다고 해석\n",
    "* Smooth function - 무한번 미분이 가능한 함수를 말한다. 미분 가능은 연속적이어야 하고 그점에서 미분 계수가 같으므로 smooth 하게 연결됨을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e0f56",
   "metadata": {},
   "source": [
    "## Sigmoid \n",
    "\n",
    "feedforward neural networks에서 사용되는 비선형 활성화 함수  \n",
    "딥러닝에서 보통 output layer에서 사용되며, output에 대한 확률을 예측하는데 사용된다. 출력값의 범위로 0~1 사이로 제한된다. \n",
    "\n",
    "------ \n",
    "\n",
    "* 장점 \n",
    "* binary classification 문제, logistic regression 모델링, 기타 neural network 도메인 등에서 성공적으로 적용되었음.\n",
    "* 이해 하기 쉽고 대부분의 shallow network에서 사용됨\n",
    "* small random 값들에 의해 neural network를 초기화시키는 경우를 피할 수 있음  \n",
    "    \n",
    "    \n",
    "* 단점\n",
    "* hidden layers에서 input layers로 gradient를 backpropagation 시 sharp하고 damp한 gradient를 가짐, gradient saturation, 수렴이 느림, 출력의 중심이 0이 아님” 등의 이유로 gradient update 시 다른 방향으로 propagation됨.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "기존에 사용하던 Simgoid fucntion을 ReLu가 대체하게 된 이유 중 가장 큰 것이 Gradient Vanishing 문제이다. Simgoid function은 0에서 1사이의 값을 가지는데 gradient descent를 사용해 Backpropagation 수행시 layer를 지나면서 gradient를 계속 곱하므로 gradient는 0으로 수렴하게 된다. 따라서 layer가 많아지면 잘 작동하지 않게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70857aa8",
   "metadata": {},
   "source": [
    "## Tanh \n",
    "\n",
    "zero-centered function으로서, 출력 값의 범위는 -1~1 사이로 제한됨.\n",
    "\n",
    "------\n",
    "\n",
    "* 장점\n",
    "* multi-layer neural network에 대한 훈련 성능이 더 좋기 때문에 Sigmoid function보다 선호되는 function임\n",
    "* 출력의 중심이 0이기 때문에 back-propagation 처리를 도울 수 있음.  \n",
    "\n",
    "\n",
    "* 단점 \n",
    "* Sigmoid function과 유사하게 vanishing gradient 문제를 해결하지 못함.\n",
    "* 입력 값이 0일 때만 gradient가 1이 되며, 이는 계산 중에 dead neuron을 일부 생성하게 함.\n",
    "* dead neuron은 gradient가 0이 되어 activation weight가 사용되지 않는 상태를 말하며, 이러한 한계로 인해 Rectified Linear Unit(ReLU) activation function이 탄생됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661819a",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "실수 값을 가지는 vector로부터 확률 분포를 계산하는데 사용되며, 0~1 사이의 출력 값을 가짐  \n",
    "\n",
    "각 class에 대한 확률을 반환하는 multi-class model에 사용되며, target class는 가장 높은 확률을 갖게 됨. deep learning 아키텍처의 output layer에서 주로 사용됨  \n",
    "\n",
    "Sigmoid와 Softmax의 가장 큰 차이는 Sigmoid는 binary classification에서 사용되는 반면에 Softmax는 multivariate classification task에서 사용된다는 점임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95312359",
   "metadata": {},
   "source": [
    "## ReLU \n",
    "\n",
    "ReLu는 입력값이 0보다 작으면 0이고 0보다 크면 입력값 그대로를 내보냄\n",
    "\n",
    "---\n",
    "\n",
    "* 장점 \n",
    "*  Sparse activation : 0이하의 입력에 대해 0을 출력함으로 부분적으로 활성화 시킬수 있다.\n",
    "* . Efficient gradient propagtion : gradient의 vanishing이 없으며 gradient가 exploding 되지 않는다.\n",
    "* Efficient computation : 선형함수이므로 미분 계산이 매우 간단하다.\n",
    "\n",
    "\n",
    "* 단점 \n",
    "* ReLU의 overfitting 영향을 줄이기 위해 dropout 기법이 적용되고 rectified networks가 deep neural network의 성능을 향상시키긴 했지만 igmoid function에 비해 쉽게 overfitting 되는 한계가 있음.\n",
    "* 일부 gradient가 죽어버리기 때문에 훈련이 지속적으로 되지 않는 분명한 한계가 있음. 이로 인해 일부 neuron들이 dead 상태가 되어 향 후 data points에서 weight update가 활성화 되지 않고, dead neuron이 zero activation을 제공하기 때문에 학습에 방해를 야기함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94386a6",
   "metadata": {},
   "source": [
    "## Swich \n",
    "\n",
    "hybrid activation function을 만들기 위해 sigmoid activation function과 input function의 조합으로 제안된 최초의 compound activation function임.   \n",
    "\n",
    "function을 계산하기 위해 reinforcement learning 기반의 automatic search 기법을 사용함. \n",
    "\n",
    "smoothness, non-monotonic, 아래는 경계가 있으며(bounded below), 상한은 경계가 없는(unbounded in the upper limits) 성질을 가지고 있음. smoothness 속성은 swish function이 deep learning 아키텍처를 훈련 할 때 보다 나은 최적화(optimization) 및 일반화(generalization) 결과를 생성할 수 있도록 해줌  \n",
    "\n",
    "* 훈련 동안에 우수한 information propagation을 제공하며, vanishing gradient 문제를 겪지 않기 때문에 단순함과 향상된 정확성을 가지게 됨. deep learning 기반 classification task에서 ReLU activation function 보다 우수한 결과를 보여줌.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4b361",
   "metadata": {},
   "source": [
    "## GELU (Ganussian Error Linear Unit) \n",
    "\n",
    "GELU 함수는 dropout, zoneout, ReLU 함수의 특성을 조합하여 유도 되었다. 먼저 ReLU함수는 입력 x의 부호에 따라 1이나 0을 deterministic하게 곱하고 dropout은 1이다 0을 stochastic하게 곱하게 된다. 따라서 GELU에서는 이 두개념을 합쳐 0 또는 1로 이루어진 마스크를 stochastic하게 곱하면서도 stochasticity를 x의 부호가 아닌 값에 의해서 정하고자 한다.\n",
    "\n",
    "$ x*1/2 [1+ erf + (x / \\sqrt2) ] $ -> $x*sigmoid(1.70*2x)$ , erf = error function  \n",
    "\n",
    "GELU 함수는 모든 점에서 미분 가능하고 단조증가함수가 아니다. 이는 비선형 활성화 함수 목적에 맞게 더욱 복잡한 전체 함수를 모델링하는데 도움이 된다. 또한 ReLU함수는 x의 부호에 대해서 gating되는 것과 달리 GELU함수는 x가 다른 입력에 비해서 얼마나 큰지에 대한 비율로 gating되니 확률적인 해석이 가능해지고 함수 형태가 미분가능하게 된다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c97635",
   "metadata": {},
   "source": [
    "### 전체적인 요약 \n",
    "\n",
    "Sigmoid / tanh 활성화 함수가 bounded 되어 있어 vanishing gradient 현상을 유발하고 이를 대체하기 위해 ReLU 함수가 널리 사용되고 있지만 ReLU는 입력이 음수가 되어버리는 순간 기울기가 0이므로 그 노드에 연결된 파라미터들이 업데이트가 되지 않는다. 이러한 현상이 neural networks에서 자주 발생해 neuron 대부분이 0이 되어버리는 sparsity가 발생할 때는 dying ReLU 현상이라고 한다. 이 경우는 자주 일어나지는 않지만 매우 큰 lr를 갖거나 매우 큰 움수 bias 항을 가지는 경우에 ReLU 입력이 급격하게 음수가 되어 버릴 때 주로 발생한다.  \n",
    "\n",
    "\n",
    "이를 극복하기 위해 Leaky ReLU 같이 음수 부분에 작은 기울기를 주는 활성화 함수가 개발이 되었지만 이 함수들은 음수에 대해서bounded (bounded below )되어있지 않기 때문에 feature 학습에 부정적인 영향을 끼질 가능성이 있다.(음수가 곱해지는 것이 누적되면 최종적인 활성화가 잘 되지 않는다.) 따라서 GELU, Mish, Swish 같이 음수 부분이 값을 가지지만 너무 커지지 않게 bounded하면서 기울기가 잘 정의되어 있는 함수가 점점 넓게 사용되고 있다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
