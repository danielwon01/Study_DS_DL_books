{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e782ecb4",
   "metadata": {},
   "source": [
    "## xception 모델\n",
    "\n",
    "Xception은 구글이 2017년에 발표한 모델\n",
    "* Encoder-Decoder 형태의 모델들에서 pretrain 된 Xception 모델이 Encoder로 자주 쓰입니다. 또한 Xception 에서 제시하는 모델의 구조나 핵심인 'modified depthwise separable convolution'의 개념이 간단하기 때문에 다른 모델에도 적용하기 쉽습니다.  \n",
    "\n",
    "\n",
    "* Xception 이라는 이름 자체가 Extreme + Inception 에서 나온 만큼 Inception 모델이 기본"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afed27c",
   "metadata": {},
   "source": [
    "#### Inception 모델 \n",
    "\n",
    "딥러닝은 망이 깊을수록 (deep), 레이어가 넓을수록 (wide) 성능이 좋지만, overfitting & vanishing gradient 의 문제로 깊고 넓게만 모델을 만드는 것은 문제가 됩니다. Inception 은 Convolution 레이어를 sparse 하게 연결하면서 행렬 연산은 dense하게 처리하기 위해 고안한 모델\n",
    "\n",
    "----\n",
    "\n",
    "#### 모델 구조 \n",
    "\n",
    "<img src=\"https://nbviewer.org/github/Hyunjulie/KR-Reading-Image-Segmentation-Papers/blob/master/images/naive.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "보통 5x5 또는 7x7의 하나의 convolution 필터로 진행하는데, Inception 모델에서는 conv 레이어 여러 개를 한 층에서 구성하는 형태를 취하고 있다.\n",
    "conv 레이어를 여러개 사용하는 이유는 Parameter의 개수도 줄이고, 연산량도 줄일 수 있기 때문이다.   \n",
    "\n",
    "Kernel size가 늘어날수록 연산량의 크기가 굉장히 커지기 때문에 나중에는 5x5 가 아니라 3x3을 2번 하는 방향으로 간다. 그 이후  3x3 를 쪼개서 3x1 과 1x3 convolution 을 2번하는 방향으로 가기도 한다. (Asymmetric Convolution Factorizing )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c03a25",
   "metadata": {},
   "source": [
    "#### Xception - Modified Depthwise Separable Convolution \n",
    "\n",
    "Modified 전 Depthwise Separable Convolution 모델 설명 \n",
    "\n",
    "Depthwise(깊이 or 채널별로) 나누어서 conv를 하는 것 \n",
    "\n",
    "<img src=\"https://nbviewer.org/github/Hyunjulie/KR-Reading-Image-Segmentation-Papers/blob/master/images/original%20depthwise%20convolution.png\" width=\"400\" height=\"300\"/>  \n",
    "\n",
    "1. Channel-wise nxn spatial convolution  \n",
    "\n",
    "   그림에서와 같이 인풋으로 5개의 채널이 들어오면 5개의 n x n convolution 을 따로 진행해서 합칩니다.  \n",
    "   \n",
    "   \n",
    "\n",
    "2.  Pointwise Convolution\n",
    "    원래 우리가 알고있는 1x1 convolution입니다. 채널의 개수를 줄이기 위한 방법으로 사용됩니다.\n",
    "\n",
    "      예시: 밑에와 같이 256 x 256 x 3 의 인풋이 있을 때\n",
    "\n",
    "<img src=\"https://nbviewer.org/github/Hyunjulie/KR-Reading-Image-Segmentation-Papers/blob/master/images/original.png\" width=\"400\" height=\"300\"/>  \n",
    "\n",
    "1단계에서는: 256 x 256 x 1 을 3번 진행해서 concat을 합니다.  \n",
    "\n",
    "\n",
    "2단계에서는: pointwise convolution을 이용해서 채널의 개수를 1개로 줄입니다 (단순하게 weighted sum 을 계산하는 것)  \n",
    "\n",
    "위와같은 과정으로 Convolution 을 하면 약 9배 정도 빠르다고 합니다.\n",
    "\n",
    "\n",
    "\n",
    "#### Xception의 Modified  \n",
    "연산 순서의 변경, Pointwise를 먼저 진행한 뒤 depthwise   \n",
    "\n",
    "Non-Linerarity의 유무, nception 모델의 경우, 첫 연산 후에 non-linearity (ReLU)가 있지만, Xception은 중간에 ReLU non-linearity 를 적용하지 않는다.  \n",
    "\n",
    "Residual connection 이 거의 모든 Layer 에 있다 -> 없애고 실험해봤더니 있을때의 정확도가 훨씬 높았음. residual connection 이 굉장히 중요한 요소임\n",
    "\n",
    "------\n",
    "\n",
    "#### Xception - Modified Depthwise Separable Convolution  모델 구조 \n",
    "\n",
    "Entry Flow  \n",
    "\n",
    "1. 인풋: 229 x 229 x 3  \n",
    "\n",
    "    모든 convolutional layer 다음에는 batch normalization 을 사용한다    \n",
    "    normal convolution (3x3) -> 필터의 갯수: 32 -> 64    \n",
    "    Residual Network 가 합쳐진 Inception Module 3번      \n",
    "    \n",
    "\n",
    "2. Middle Flow\n",
    "     반복되는 단순한 모델: 필터의 개수와 width/height 는 바뀌지 않는다  \n",
    "    ReLU -> Separable Conv -> Separable Conv 이걸 8번 반복하기  \n",
    "\n",
    "\n",
    "3. Exit Flow\n",
    "    filter의 개수를 늘린다음 -> Maxpooling -> 2번 separable convolution -> Global Average Pooling -> Optional Fully-Connected -> Logistic Regression\n",
    "\n",
    "<img src=\"https://nbviewer.org/github/Hyunjulie/KR-Reading-Image-Segmentation-Papers/blob/master/images/xception%20architecture.png\" width=\"600\" height=\"300\"/>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aa7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init \n",
    "\n",
    "# Pretrained 된 weight 를 사용할 때 필요합니다. weight 들이 담긴 파일을 다운받는 링크\n",
    "model_urls = {'xception':'https://www.dropbox.com/s/1hplpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648787",
   "metadata": {},
   "source": [
    "모델에서 기본으로 사용되는 'Convolution' -> 'Pointwise Convolution' 을 하나의 class 로 묶어서 재사용할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9d42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding =0, dilation=1, bias=False):\n",
    "        super(sepertableConv2d, self).__init__()\n",
    "        \n",
    "        # Forward 함수 정의 \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, \n",
    "                          out_channels = in_channels, # depthwise convolution 에서는 \n",
    "                          stride = stride,            # in channel과 out channel의 수가 같다 \n",
    "                          kernel_size = kernel_size,\n",
    "                          padding = padding, \n",
    "                          dilation = 1, \n",
    "                          bias = False, \n",
    "                          groups = in_channels # 중요한 부분 , channel별로이기 때문에 inchannel 의 개수와 같게 적용\n",
    "                          )\n",
    "        \n",
    "        self.pointwise = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels = out_channels, \n",
    "                              kernel_size = 1, stride = 1, padding=0, dilation=1, groups =1, bias=bias)\n",
    "            \n",
    "    def forward(self , x) :\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffd1a1",
   "metadata": {},
   "source": [
    "위 class 기반으로 모델에서 지속적으로 사용되는 Block을 만듬  \n",
    "\n",
    "\n",
    "* Parameters\n",
    "\n",
    "* reps: separable convolution (Middle Flow 에서는 3번, 그 외에는 2번)\n",
    "* start_with_relu: convolution  (첫 블록만 false 고 다 ReLU 로 시작한다)\n",
    "* grow_first: 필터의 개수가 그 블럭의 첫 convolution 을 할 때 증가하느냐 마지막 convolution 을 할 때 증가하느냐 (마지막 블록만 false 고 다 true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ec3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module) :\n",
    "    def __init__(self, in_filters, out_filters, reps, strides = 1, start_with_relu = True, grow_first = True) :\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "    # skip 은 Residual을 가르킨다\n",
    "    # 인풋과 아웃풋의 필터의 개수가 다르다면 개수를 맞춰주기 위해\n",
    "    # 필터의 개수가 맞게 convolution 을 진행해야함 -> kernel의 크기는 1로 \n",
    "    \n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:       \n",
    "            self.skip=None    #인풋과 아웃풋 필터의 개수가 같다면 조정할 필요 없음\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep=[] #모든 computation 을 rep 에 저장하기\n",
    "        \n",
    "        \n",
    "        filters = in_filters\n",
    "        if grow_first: #필터의 개수를 늘리고 시작하는 블록이라면\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1): # 블록에 Depthwise convolution이 몇번 있느냐?\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "        \n",
    "        if not grow_first: # 필터의 개수를 마지막에 늘리는 블록이라면 \n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu: #ReLU 로 시작하지 않으면 앞에 ReLU 하나 떼어내가 \n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1: # stride 가 1이 아니면 MaxPooling을 적용한다 \n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "        \n",
    "        #Residual Network의 필터개수 맞춰주기\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "        \n",
    "        #Residual 연결\n",
    "        x+=skip\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9090a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module): \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #Entry Flow 에서 쓸 함수 정의하기\n",
    "        #모든 convolution 다음에는 batch norm이 온다 \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #ReLU 넣는거 까먹지 않기!\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
    "        #Entry Flow 의 아웃풋은 19x 19 x 728 feature maps\n",
    "        \n",
    "        # Middle Flow 에서 쓸 함수: 같은거 8번 반복\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        #Middle Flow 의 아웃풋은 19 x 19 x 728 feature maps-> 크기는 같음\n",
    "\n",
    "        #Exit Flow 에서 쓸 함수\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "        #ReLU 넣는거 까먹지 않기!\n",
    "\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "        #ReLU 넣는거 까먹지 않기!\n",
    "\n",
    "        #Optional FC Layer \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        #------- init weights --------\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        #-----------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09991e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception(pretrained=False,**kwargs):\n",
    "\n",
    "    model = Xception(**kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['xception']))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
