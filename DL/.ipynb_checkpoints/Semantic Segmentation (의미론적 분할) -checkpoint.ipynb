{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0e005d",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*cJ3oLJ2s8W_sCPmTuMgIlw.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "* Classification (분류): 인풋에 대해서 하나의 레이블을 예측하는 작업.   \n",
    "  AlexNet, ResNet, Xception 등의 모델   \n",
    "  \n",
    "  \n",
    "* Localization/Detection (발견): 물체의 레이블을 예측하면서 그 물체가 어디에 있는지 정보를 제공. 물체가 있는 곳에 네모를 그리는 등\n",
    "    YOLO, R-CNN 등의 모델  \n",
    "    \n",
    "    \n",
    "* Segmentation (분할): 모든 픽셀의 레이블을 예측\n",
    "    FCN, SegNet, DeepLab 등의 모델\n",
    "    \n",
    "이미지 내에 객체 존재 여부를 예측하는 문제(이미지 분류; Image Classification)에 비해서 객체 경계 정보를 보존해야하고 전체적인 이미지의 문맥을 파악해야 하는 등 조금 더 높은 수준의 이미지 이해를 요구한다는 점에서 까다로운 문제에 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99fba5",
   "metadata": {},
   "source": [
    "<img src=\"https://images.velog.io/images/cosmicdev/post/80eb0f27-8690-44f7-9eaf-0083c0c5caa0/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-10-25%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%203.02.45.png\n",
    "\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Semantic Image Segmentation의 목적은 사진에 있는 모든 픽셀을 해당하는 (미리 지정된 개수의) class로 분류하는 것이다. 이미지에 있는 모든 픽셀에 대한 예측을 하는 것이기 때문에 desce prediction이라고도 불린다.    \n",
    "\n",
    "주의할 점은 Semantic Image Segmentation은 같은 class의 instance를 구별하지 않는다는 것이다. class에 속하는 object이 있을 때 따로 분류하지 않고 그 픽셀 자체가 어떤 class에 속하는지에만 관심이 있다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8e314",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*p-F6uIUf6jmcM8VcVQarGQ.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "인풋으로 RGB이미지 (H X W X 3) 또는 흑백이미지 (H X W X 1) , 아웃풋으로는 각 픽셀별 어느 class에 속하는 나타내는 레이블을 나타낸 Segmentation Map. \n",
    "\n",
    "-----\n",
    "\n",
    "<img src=\" https://miro.medium.com/max/1400/1*lZEdphf72ZMwOhsDq-eEfg.png\" width=\"500\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "One-Hot-encoding으로 각 class에 대해 출력채널을 만들어서 segmentation map을 만든다. class의 개수 만큼 만들어진 채널을 argmax를 통해서 위에 이미지 처럼 하나의 출력물을 도출한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf0177",
   "metadata": {},
   "source": [
    "#### Semantic Segmentation 방법들 \n",
    "\n",
    "AlexNet, VGG 등 분류에 자주 쓰이는 깊은 신경망들은 Semantic Segmentation 을 하는데 적합하지 않다. 일단 이런 모델은 parameter 의 개수와 차원을 줄이는 layer 를 가지고 있어서 자세한 위치정보를 잃게 되기 때문이다. 또한 보통 마지막에 쓰이는 Fully Connected Layer에 의해서 위치에 대한 정보를 잃게 된다.\n",
    "\n",
    "만약 공간/위치에 대한 정보를 잃지 않기 위해서 Pooling 과 Fully Connected Layer 를 없애고 stride 가 1이고 Padding 도 일정한 Convolution 을 진행할 수도 있을 것이다. 인풋의 차원은 보존하겠지만, parameter 의 개수가 많아져서 메모리 문제나 계산하는데 비용이 너무 많이 들어서 현실적으로는 불가능 할 것이다. \n",
    "\n",
    "이 문제의 중간점을 찾기 위해 보통 Semantic Segmentation 모델들은 보통 Downsampling 과 Upsampling의 형태를 가지고 있다. \n",
    "\n",
    "\n",
    "<img src=\" https://miro.medium.com/max/3020/1*3xgDGoeO36zjycZFHFgvCA.png\" width=\"600\" height=\"300\"/>  \n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "* Downsampling - 주 목적은 차원을 줄여서 적은 메모리로 깊은 Convolution 을 할 수 있게 하는 것이다. 보통 stride 를 2 이상으로 하는 Convolution 을 사용하거나, pooling을 사용한다. 이 과정을 진행하면 어쩔 수 없이 feature 의 정보를 잃게된다. \n",
    "\n",
    "\n",
    "* 마지막에 Fully-Connected Layer를 넣지 않고, Fully Connected Network 를 주로 사용한다. FCN 모델에서 위와같은 방법을 제시한 후 이후에 나온 대부분의 모델들에서 사용하는 방법이다. \n",
    "\n",
    "\n",
    "* Upsampling - Downsampling 을 통해서 받은 결과의 차원을 늘려서 인풋과 같은 차원으로 만들어 주는 과정이다. 주로 Strided Transpose Convolution 을 사용한다.\n",
    "\n",
    "\n",
    "* 논문들에서 Downsampling 하는 부분을 인코더, Upsampling 하는 과정을 디코더이라고 부른다.인코더를 통해서 입력받은 이미지의 정보를 압축된 벡터에 표현하고, 디코더를 통해서 원하는 결과물의 크기로 만들어낸다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d0d162",
   "metadata": {},
   "source": [
    "#### Fully Convolutional Network for Semantic Segmentation FCN \n",
    "\n",
    "FCN은 이미지 분류에서 우수한 성능을 보인 CNN 기반 모델(AlexNet, VGG16, GoogLeNet)을\n",
    "Semantic Segmentation Task를 수행할 수 있도록 변형시킨 모델,FCN 네트워크는 이미지 분류 문제를 먼저 트레이닝 시킨 후 모델을 튜닝해 학습하는 Transfer Learning으로 구현  \n",
    "\n",
    "\n",
    "FCN은 기존 이미지 분류에서 쓰인 네트워크를 트레인된 상태에서(Pretrain)\n",
    "Feature Extraction 레이어는 그대로 재활용하여 Feature를 추출하고\n",
    "FC 레이어를 버리고 1X1 Conv 그리고 Up-sampling(Transpose Convolution)로 변경하여(Fine-Tuning)\n",
    "픽셀 클래스 분류와 입력이미지와 같은 사이즈 회복을 하도록 네트워크가 구성되었다. \n",
    "\n",
    "<img src=\" https://miro.medium.com/max/875/1*ddf0sJZDitiqVCWdjijzCA.png\" width=\"600\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "* 네트워크 구조 \n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdab59q%2Fbtq0tfkffFq%2Fk4GD8TKLusWjfVq36DMBn0%2Fimg.png\n",
    "\" width=\"600\" height=\"300\"/>  \n",
    "\n",
    "1. Convolution Layer를 통해 Feature 추출\n",
    "2. 1x1 Convolution Layer를 이용해 피처맵의 체널수를 데이터셋 객체의 개수와 동일하게 변경\n",
    "    (Class Presence Heat Map 추출)\n",
    "3. Up-sampling: 낮은 해상도의 Heat Map을 Upsampling(=Transposed Convolution) 한 뒤, 입력 이미지와 같은 크기의 Map 생성\n",
    "4. 최종 피처 맵과 라벨 피처맵의 차이를 이용하여 네트워크 학습\n",
    "\n",
    "\n",
    "\n",
    "----- \n",
    "* 문제점 존재 \n",
    "\n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbb7uvh%2Fbtq0zdkIsTF%2FQWkZ87uNnbUqjb505VldH0%2Fimg.png\" width=\"300\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "VGG16에서 입력 이미지의 크기가 224x224 인 경우 5개의 convolution block을 통과하면 Feature map의 크기는 7x7이 된다. 마찬가지로 FCN에서도 기존 입력 이미지 (크기 H x W)가 5개의 convolution block을 통과하면\n",
    "H/32 x W/32 크기의 Feature map을 얻게 된다.\n",
    "\n",
    "Feature map의 한 픽셀은 입력 이미지의 32 x 32 pixel를 대표하게 된다.\n",
    "이처럼 Feature map은 낮은 해상도를 가져 입력 이미지의 위치 정보를 '대략적으로만' 가지고 있다.\n",
    "\n",
    "문제는 3번 Up-sampling 과정에서 발생한다.\n",
    "입력 이미지 위치 정보를 '대략적으로' 가지고 있는 feature map을 Up-sampling하여 얻은 segmentation map은\n",
    "기존 입력 이미지와 비교했을 때 뭉뚱그려져 있고 디테일하지 못하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c07e6",
   "metadata": {},
   "source": [
    "#### Skip architecture\n",
    "FCN의 개발자들은 좀 더 디테일한 segmentation map을 얻기 위해, Skip architecture 라는 기법을 제안  \n",
    "\n",
    "앞에서 설명과 같이 최종 피처맵(Feature map)은 지역정보를 '대략적으로 유지'하고 있어 이미지가 뭉개지는 것을 보완한 방법이다. 피처 추출 단계의 피처맵도 업샘플링에 포함(Sum)하여 위치 정보 손실을 막자라는 것이 핵심 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d9b1c",
   "metadata": {},
   "source": [
    "#### FCN 확장 모델\n",
    "\n",
    "* FCN-32s  \n",
    "* 5번의 convolution block을 통과해 1/32만큼 줄어든 5번 Feature map(5번 Feature map 크기는 7x7)\n",
    "* 5번 Feature map이 convolution layer를 통과하여 같은 크기의 6번 Feature map을 얻음\n",
    "* 6번 Feature map을 한 번에 32배 upsampling (H/32 x W/32 크기를 H x W 크기로)\n",
    "\n",
    "*  FCN-16s\n",
    "* 4번의 convolution block을 통과해 1/16 만큼 줄어든 4번 Feature map, 4번 Feature map 크기는 14x14\n",
    "* 6번 Feature map을 2배 upsampling (H/32 x W/32 크기를 H/16 x W/16 크기로) 한 것과 4번 Feature map을 Sum 한다.(4-1번 Feature map)\n",
    "* 새롭게 얻은 4-1번 Feature map 을 한 번에 16배 upsampling (H/16 x W/16 크기를 H x W 크기로)\n",
    "\n",
    "\n",
    "<img src=\" https://wikidocs.net/images/page/143443/FCN-16S.png\" width=\"500\" height=\"300\"/>    \n",
    "\n",
    "\n",
    "* FCN-8s\n",
    "\n",
    "* FCN-16s과 유사한 Step이 추가됩니다.\n",
    "* 4-1번 Feature map을 2배 upsampling(H/16 x W/16 크기를 H/8 x W/8 크기로) 한 것과 3번 Feature map을 Sum한다.(3-1번 Feature map)\n",
    "* 3-1번 Feature map을 한 번에 8배 upsampling (H/8 x W/8 크기를 H x W 크기로)\n",
    "\n",
    "<img src=\" https://wikidocs.net/images/page/143443/FCN-8S.png\" width=\"500\" height=\"300\"/>    \n",
    "\n",
    "결과 이미지를 보면 위치 정보가 더 잘 전달되어 FCN-32s → FCN-16s → FCN-8s 순서로 더 정교해졌음을 알 수 있다. \n",
    "\n",
    "<img src=\" https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FczpfKI%2Fbtq0vPkz77r%2F9vhp9EICvkrgtKTPrUtTM0%2Fimg.png\" width=\"300\" height=\"300\"/>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd1ef8",
   "metadata": {},
   "source": [
    "####  Dilated/Atrous Convolution 을 사용하는 방법\n",
    "\n",
    "Deeplab 이 제시하는 방법으로, 신호가 소멸되는것을 제어하고 다양한 크기의 특징을 익히는 방법을 제시  \n",
    "Atrous Convolution 은 dilation rate (확장 비율)이라는 새로운 변수를 사용한다. 이 비율은 커널에서 사용할 값들 사이에 얼마 만큼의 공간을 넣어줄 것인지에 대한 것이다. 커널 안에 공간이 없는 일반적인 convolution 은 확장비율은 1로 정의한다. 3 x 3 의 크기의 커널이 2의 확장비율을 갖는다면 실제로는 5 x 5 커널의 시야를 갖게 된다. 위와 같은 방법은 동일한 계산 비용으로 보다 넓은 시야를 갖을 수 있게 한다.  \n",
    "\n",
    "<img src=\" https://miro.medium.com/max/1400/1*bWchlYbT_81cQLzU8rOSSQ.png\" width=\"300\" height=\"300\"/>    \n",
    "\n",
    "\n",
    "----- \n",
    "\n",
    "Deeplab V3는 ImageNet에서 학습된 ResNet을 기본적인 특징 추출기로 사용한다. ResNet의 마지막 블럭에서는 여러가지 확장비율을 사용한 Atrous Convolution을 사용해서 다양한 크기의 특징들을 뽑아낼 수 있도록 한다.  \n",
    "  \n",
    "또한 이전 Deeplab 버젼에서 소개되었던 Atrous Spatial Pyramid Pooling (ASPP)을 사용한다. 새로운 아이디어라기 보다는, 좋은 성능을 보였던 모델들의 특징들을 섞어놓은 모델이다. 다양한 확장비율을 가진 커널을 병렬적으로 사용한 convolution 이다.\n",
    "\n",
    "<img src=\" https://miro.medium.com/max/1400/1*p6wpU7klyK-vGqISNumtlg.png\" width=\"600\" height=\"300\"/>    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
